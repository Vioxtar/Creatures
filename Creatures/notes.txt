
	SATURATION IS LIFE
	OPACITY IS ENERGY
	RADIUS IS MEAT

	MEAT IS STORED ENERGY, RAISES WHEN MAXIMUM ENERGY IS FILLED AND WE KEEP EATING
	WHEN ENERGY IS BELOW MAXIMUM MEAT SLOWLY TURNS TO ENERGY



	Sensors approach

	Motivations
		1. sensing the world does not require an exploding amount of brain inputs for performance reasons
		2. creatures are able to gather a rich collection of data that is sufficient to describe their enviornment
		3. creatures and the end user see the same data: we dont want the user to be able to see more than creatures, this can lead to an extreme gap between the user's expectations of creature performance to actual creature performance, which results in an overall disappointing user experience
		4. the act of rewarding sensing can be 'learned' through evolution

	Solution
		Instead of relying on cones or traces, we rely on sampling of data that corresponds to a single point in space within a creature's sense radius
		Much like our own eyes that can move independently of our head/body, creatures will be able to control sensor podisitions within the sense radius
		Every sense point returns:
			0. Activation - 1 if extremely close, 0 if really far, or none-existant! Basically the fraction of our sense radius!
			1. last hue difference (species) (the distance between their hue and our hue, thus always between 0 and 1)
			2. last saturation (life) (a value between 0 and 1, activated)
			3. last lightness (communication) (a value between 0 and 1, activated)
			4. energy (?)
			5. spike  (actual value times dot to see if aiming at us)
			6. feeder (actual value times dot to see if aiming at us)
			7. shield (actual value times dot to see if aiming at us)
			8. perhaps some relative velocity?
			9. perhapse some data sensed creature's position / what side of it did we sample?

		Every sensing points' relative position can be described via the following options:
			1. distance fraction in the sense zone, and angle
			2. euclidean space remapped to area
			3. idk

	Problems
		1. its unclear how we should be moving sensing points left/right using our brain outputs without somehow requiring two nodes for left/right and a third for up down
		2. this may necessitate non-activated outputs, but maybe we'll probably be doing that anyways
		3. controlling sensing point placement can be quite complex - and probably isnt as rewarding as a traces/cones approach in its un-evolved state (to gain useful vision, a good portion if not all of the brain may be required, and what we're left is may not be enough.
		   it can be argued that a certain fusion between creature functionality and useful eyesight can be achieved, but this HAS to come at a cost of creature behavior)
		   we are not here to observe how creatures use their eyes! we're here to see epic swarms and group dynamics... does this require a complicated vision model?
		4. it is difficult to avoid a O^(creatures in tile * number of sensing points * creatures in tile) complexity, we may need to come up with a clever spatial indexer of some sort


	When considering the sensor circle/points approaches, we need to choose between:
		1. Less memory access, more computations: bigger uniform grid tile of sense radius dimensions, during in-tile iterations test for senses
		2. More memory access, less computations: we spatial-index the uniform grid based on our sampling positions: but then we STILL have to perform
			computations to test for sensing, only on far less creatures...

	When considering the cone/line approaches, sensing areas span across several uniform tiles so we're forced to go with the first 'less memory access' choice

	One of these options is probably more performant than the other... and its probably the first...? because we're indexing the uniform grid incoherently creature_count * sensor_count times
	Choose first option!


	NON-OBVIOUS-TODOS:
	ADD A GENERATIONS HISTOGRAM (instead of average generations etc)
	For removal of creatures you have two options:
		1. Keep a isValid SSBO and bind that to EVERY program and only compute stuff if its valid..., plus keep a stack of 'nonvalid' creature indices to be used should we want to add a new creature (in which case we simply recycle old buffer memory)
		2. Do a replace-with-last style removal, but the problem then is that if the UI had the last creature selected, the selction would immediately change to the dying creature only for it to be removed..
			to fix this^ we'll need to keep a UniqueIDs ssbo, and have the UI store that uniqueID instead. To keep the uniqueID alligned with actual indices (which is useful!) we can simply edit the uniqueIDs of creatures on the switch removal action!
	Consider adding negated activated functions (like you posted in discord) for maybe even MORE expressive brains!
		But for now... use two-node setups for moving shit linearly
	Create two creature-creating functions: create first gen creature, and create offspring creature
	replace branching code with branchless code
	Consider giving every node its own activation exponent?
	Find if theres a way to pass non-interpolated data to the fragment shader by creature instance
	Implement 'visibilities' SSBO, every frame update each creature's 'LOD' value: false is offscreen, true is visible, then use that for deformations/whatever else
	(Optional) Refactor simulation.cpp by splitting up code (programs, creature SSBOs, etc, just like camera is split up)
	Settings pretty names
	DearImgui integration and UI
		F11 to fullscreen
	Add WINDOW settings in settings file?
	Spikes and shields and stick mechanisms
	Fix window misalignment
	FIX THE WEIRD GHOST BALL AT 0, 0
	Find out why border physics is so damn unstable (not really the case when border restitution is at 0)





	Some notes
	
		RadiusPercentage = 0.3 * LifePercentage + 0.5 * EnergyPercentage + 0.2 * ControlPercentage

	saturation belongs to family
	life reflected by fill

	make stickyness global to creature's body, make A not stick to B if A deflects B with shield or if B deflects A with shield..?

	Creatures control hardness target, and have a stick mechanism (either arms or body or some sort of friction side)

	Sigmoid activation can be described as:

	sigmoidActivation(x) =
		if x == 0.5 then return x
		if x < 0.5 then return ((2x)^a)/2
		if x > 0.5 then return -(((2(1-x))^a) / 2) + 1

	for some constant a = 8-ish

	It could also probably be approximated further by dividing into regions and returning linear functions within those regions

















	Approach to brains:
	We wish to minimize memory complexity of brains with minimal time complexity tradeoffs.
	
	Assume that:
	1. We have about 2 GB of usage space for brains
	2. We want to support up to 100,000 creatures at once

	Some numbers:
		1 GB =~ 250,000,000 floats
		250,000,000 / 100,000 = 2500 floats per creature
		However we about just about 2 GB available, so that's 500,000,000 floats / 100,000 creatures = 5000 floats per creature.
		Going by that number, that gives us just about 4500 floats per brain, leaving us with 500*4 bytes for other attributes.
		

	Some more numbers:
		If we would have 40 nodes in a level, and up to 5 levels, then every link level would have 40^2 = 1600 links, meaning 1600*5 = 8000 total links in the brain. If every link has a scalar float and a bias float, then that's 16000 floats per brain!
		A more feasible scenario is have 60 input nodes, 10 nodes in every hidden level, and 5 levels, that means: 60*10 + 10*10*4 = 1000 links in the entire brain! 2 floats in a link implies 2000 floats, and were we to consider node values, then that's 110 more floats = 2110 floats per brain.
	
		If a sensor each has:
			1. Activation
			2. Hue
			3. Lightness
			4. Saturation
		
		And we have 8 sensors, then that's 4*8 = 32 inputs. 

	To minimize memory complexity we can take several approaches:

		a. Instead of merely defining # of nodes per level (which is always a very big number since inputs is about 40-50 nodes), define # of nodes per hidden level and # of nodes per input level, this will drastically decrease size!
		b. Instead of having every node connect to every other node in the previous layer, have it only connect to X nodes
		c. Don't use biases! Just scalars...

	The overall approach for brains:

		A single brain buffer would contain:

			[STRUCTURE HEADER UINTS | A SEQUENCE OF ALL NODES' CURRENT VALUES | A SEQUENCE OF LINKS]

		The structure header is simply a sequence of uints that tells us the structure of the brain.
			[NUMOFLEVELS = 5, 32, 15, 10, 18, 12] for example means 32 inputs, 3 middle levels (15, 10 and 18 nodes), and 12 outputs.

		The sequence of notes is merely the current values stored at each node. We know that this buffer always starts immediately after
		the structure header, and is exactly sum(structure values) indices long.

		Afterwards, a sequence of links: the length of this buffer is exactly 32*15 + 15*10 + 10*18 + 18*12 indices long.

	We can use this layout to cleverly iterate our brains and perform forward propagations.
	While this layout is borderline dynamic, we must make each total brain size fixed for proper SSBO indexing.
	We can choose one of two data layout approaches:
		
		[FIXED STRUCTURE | DYNAMIC NODES | DYNAMIC LINKS, empty space]
		(gives us more flexibility in how we forward propagate, less convenient for mutation logic)
	
	Or this:

		[FIXED STRUCTURE | FIXED NODES, empty space | FIXED LINKS, empty space]
		(gives us more comfort in mutation handling, less in forward propagations)

	We'll choose the first, since it may prove more flexible to changes in the future (no hard-coding of sub-buffer sizes)

	Finally we wish for a straight forward method to provide an upper bound on memory complexity of brains.
	
